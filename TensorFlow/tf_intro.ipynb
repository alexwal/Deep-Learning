{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "print node1, node2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran: [3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "# To actually evaluate the nodes, we must run the computational graph within a session.\n",
    "# A session encapsulates the control and state of the TensorFlow runtime. (Scope?)\n",
    "sess = tf.Session()\n",
    "print 'Ran:', sess.run([node1, node2])# run enough of the computational graph to evaluate node1 and node2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "sess.run(node3): 7.0\n"
     ]
    }
   ],
   "source": [
    "# We can build more complicated computations by combining Tensor nodes with operations.\n",
    "# Operations are also nodes.\n",
    "node3 = tf.add(node1, node2)\n",
    "print \"node3:\", node3\n",
    "print \"sess.run(node3):\", sess.run(node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add0: 7.5\n",
      "add1: [ 3.  7.]\n",
      "add2: 22.5\n"
     ]
    }
   ],
   "source": [
    "# A graph can be parameterized to accept external inputs, known as placeholders.\n",
    "# A placeholder is a promise to provide a value later.\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "\n",
    "# Equivalent Operations:\n",
    "# adder_node = tf.add(a,b)\n",
    "adder_node = tf.reduce_sum([a,b], axis=0) # Add corresponding rows (axis = 0) of Tensor(a | b). (axis = 1 adds cols)\n",
    "# adder_node = a + b\n",
    "\n",
    "# We can evaluate this graph with multiple inputs by using the feed_dict parameter\n",
    "# to specify Tensors that provide concrete values to these placeholders.\n",
    "print 'add0:', sess.run(fetches=adder_node, feed_dict={a: 3, b: 4.5})\n",
    "print 'add1:', sess.run(fetches=adder_node, feed_dict={a: [1,3], b: [2, 4]})\n",
    "\n",
    "# More complex:\n",
    "add_and_triple = adder_node * 3.\n",
    "print  'add2:', sess.run(fetches=add_and_triple, feed_dict={a: 3, b:4.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: None\n",
      "Ran: [ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "# In ML, we want to update the model to produce a different output on the same input.\n",
    "# Variables allow us to add trainable parameters to a graph (weights, biases).\n",
    "# They are constructed with a type and initial value:\n",
    "\n",
    "W = tf.Variable([.3], tf.float32) # weight (trainable)\n",
    "b = tf.Variable([-.3], tf.float32) # bias (trainable)\n",
    "x = tf.placeholder(tf.float32) # input\n",
    "linear_model = W * x + b\n",
    "\n",
    "# Constants are initialized when you call tf.constant, and their value can never change.\n",
    "# By contrast, variables are not initialized when you call tf.Variable.\n",
    "# To initialize all the variables in a TensorFlow program, you must explicitly call\n",
    "# a special operation as follows:\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# Sets global -->Variables<-- (W, b) to defaults:\n",
    "print 'init:', sess.run(init) # init.op_def == \"NoOp\" <-- init not a model (no computation), just global variable initializer.\n",
    "\n",
    "# It is important to realize init is a handle to the TensorFlow sub-graph that\n",
    "# initializes all the global variables. Until we call sess.run, the variables are uninitialized.\n",
    "\n",
    "# Since x is a placeholder, we can evaluate linear_model for several values of x simultaneously as follows:\n",
    "print 'Ran:', sess.run(fetches=linear_model, feed_dict={x: [1,2,3,4]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: None\n",
      "Ran: [ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable([.3], tf.float32) # weight (trainable)\n",
    "b = tf.Variable([-.3], tf.float32) # bias (trainable)\n",
    "x = tf.placeholder(tf.float32) # input\n",
    "linear_model = W * x + b\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "print 'init:', sess.run(init)\n",
    "\n",
    "print 'Ran:', sess.run(fetches=linear_model, feed_dict={x: [1,2,3,4]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 23.66\n"
     ]
    }
   ],
   "source": [
    "# To evaluate the model on training data, we need a y placeholder to provide the desired values,\n",
    "# and we need to write a loss function.\n",
    "\n",
    "\n",
    "# A loss function measures how far apart the current model is from the provided data.\n",
    "# We'll use a standard loss model for linear regression, which sums the squares of errors.\n",
    "# (error = linear_model(x) - y). We call tf.square to square that error.\n",
    "# Then, we sum all the squared errors to create a single scalar that abstracts the error of all\n",
    "# examples using tf.reduce_sum:\n",
    "\n",
    "# 1. Build computational graph\n",
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "\n",
    "# 2. Run the graph, providing values for all placeholders.\n",
    "print 'Loss:', sess.run(loss, {x: [1,2,3,4], y: [0,-1,-2,-3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unchanged: [array([ 0.30000001], dtype=float32), array([-0.30000001], dtype=float32)]\n",
      "Changed: [array([-1.], dtype=float32), array([ 1.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# We could improve this manually by reassigning the values of\n",
    "# W and b to the perfect values of -1 and 1.\n",
    "\n",
    "# NOTE: fixW, fixb are -->Operations<-- themselves, nodes!!! We must first run these nodes to reassign values to W, b.\n",
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])\n",
    "print 'Unchanged:', sess.run([W, b]) # See? The values haven't changed.\n",
    "\n",
    "# Run fix_ nodes to reassign values to W, b:\n",
    "sess.run([fixW, fixb])\n",
    "print 'Changed:', sess.run([W, b]) # Now after running these nodes, the values have been changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Finally, the Variables have good enough weights:\n",
    "print 'Loss:', sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow provides optimizers that slowly change each variable in order to minimize the loss function.\n",
    "\n",
    "# The simplest optimizer is gradient descent. It modifies each variable according to the magnitude of the\n",
    "# derivative of loss with respect to that variable. TensorFlow can automatically produce derivatives given only a\n",
    "# description of the model using the function tf.gradients.\n",
    "\n",
    "# For simplicity, optimizers typically do this for you. For example:\n",
    "\n",
    "# Create nodes in computational graph that enable training.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) # (defines how we'll optimize over loss)\n",
    "train = optimizer.minimize(loss) # doesn't do anything yet, just sets up the nodes. Will need to provide inputs.\n",
    "                                 # (when called, this will automatically increment global_step) (defines training goal)\n",
    "\n",
    "# Start the training:\n",
    "sess.run(init) # reset values to incorrect -->defaults<--!!!.\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x: [1,2,3,4], y: [0,-1,-2,-3]}) # one step of computation (on all? inputs)\n",
    "\n",
    "print sess.run([W, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration: 0) W: [-0.21999997] b: [-0.456] loss: 4.01814\n",
      "(Iteration: 100) W: [-0.84270465] b: [ 0.53753263] loss: 0.14288\n",
      "(Iteration: 200) W: [-0.95284992] b: [ 0.86137295] loss: 0.0128382\n",
      "(Iteration: 300) W: [-0.98586655] b: [ 0.95844597] loss: 0.00115355\n",
      "(Iteration: 400) W: [-0.99576342] b: [ 0.98754394] loss: 0.000103651\n",
      "(Iteration: 500) W: [-0.99873012] b: [ 0.99626648] loss: 9.3124e-06\n",
      "(Iteration: 600) W: [-0.99961936] b: [ 0.99888098] loss: 8.36456e-07\n",
      "(Iteration: 700) W: [-0.99988592] b: [ 0.9996646] loss: 7.51492e-08\n",
      "(Iteration: 800) W: [-0.99996579] b: [ 0.99989945] loss: 6.75391e-09\n",
      "(Iteration: 900) W: [-0.99998969] b: [ 0.99996972] loss: 6.12733e-10\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "W = tf.Variable([.3], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Loss\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss) # (when run, this will automatically increment global_step)\n",
    "\n",
    "# Training data\n",
    "x_train = [1,2,3,4]\n",
    "y_train = [0,-1,-2,-3]\n",
    "\n",
    "# Training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # reset values to wrong\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x:x_train, y:y_train})\n",
    "    \n",
    "    # Evaluate training accuracy\n",
    "    if i % 100 == 0:\n",
    "        curr_W, curr_b, curr_loss  = sess.run([W, b, loss], {x:x_train, y:y_train})\n",
    "        print(\"(Iteration: %s) W: %s b: %s loss: %s\" % (i, curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tf.contrib.learn (Estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.contrib.learn is a high-level TensorFlow library that simplifies the mechanics of machine learning,\n",
    "# including the following:\n",
    "# * running training loops\n",
    "# * running evaluation loops\n",
    "# * managing data sets\n",
    "# * managing feeding\n",
    "# * tf.contrib.learn defines many common models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': None, '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11ac58a90>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpcHQS9j\n",
      "\n",
      "\n",
      "Fitting estimator:\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From /Library/Python/2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpcHQS9j/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.5, step = 1\n",
      "INFO:tensorflow:global_step/sec: 869.612\n",
      "INFO:tensorflow:loss = 0.0250507, step = 101 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 1030.62\n",
      "INFO:tensorflow:loss = 0.0046928, step = 201 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 990.896\n",
      "INFO:tensorflow:loss = 0.00238512, step = 301 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 862.536\n",
      "INFO:tensorflow:loss = 0.000735585, step = 401 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 668.799\n",
      "INFO:tensorflow:loss = 0.000104327, step = 501 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 950.497\n",
      "INFO:tensorflow:loss = 3.08834e-05, step = 601 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 1025.13\n",
      "INFO:tensorflow:loss = 3.25561e-06, step = 701 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1041.35\n",
      "INFO:tensorflow:loss = 2.41284e-07, step = 801 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1413.95\n",
      "INFO:tensorflow:loss = 1.2645e-07, step = 901 (0.070 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpcHQS9j/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.75908e-08.\n",
      "\n",
      "\n",
      "Evaluating estimator:\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From /Library/Python/2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-05-27-18:51:54\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpcHQS9j/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-05-27-18:51:56\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 3.58368e-08\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "\n",
      "\n",
      "Done:\n",
      "\n",
      "{'loss': 3.5836759e-08, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "# Notice how much simpler the linear regression program becomes with tf.contrib.learn:\n",
    "\n",
    "# Declare list of features. We only have one real-valued feature. There are many\n",
    "# other types of columns that are more complicated and useful.\n",
    "features = [tf.contrib.layers.real_valued_column(column_name=\"x\", dimension=1)]\n",
    "\n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types like linear regression,\n",
    "# logistic regression, linear classification, logistic classification, and\n",
    "# many neural network classifiers and regressors. The following code\n",
    "# provides an estimator that does linear regression.\n",
    "estimator = tf.contrib.learn.LinearRegressor(feature_columns=features) # automatically handles W, b variables unlike^^\n",
    "\n",
    "# TensorFlow provides many helper methods to read and set up data sets.\n",
    "# Here we use `numpy_input_fn`. We have to tell the function how many batches\n",
    "# of data (num_epochs) we want and how big each batch should be.\n",
    "x = np.array([1., 2., 3., 4.])\n",
    "y = np.array([0., -1., -2., -3.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn(x={\"x\": x},\n",
    "                                              y=y,\n",
    "                                              batch_size=4,\n",
    "                                              num_epochs=1000)\n",
    "\n",
    "# We can invoke 1000 training steps by invoking the `fit` method and passing the\n",
    "# training data set.\n",
    "print '\\n\\nFitting estimator:\\n\\n'\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "\n",
    "# Here we evaluate how well our model did. In a real example, we would want\n",
    "# to use a separate validation and testing data set to avoid overfitting.\n",
    "print '\\n\\nEvaluating estimator:\\n\\n'\n",
    "done = estimator.evaluate(input_fn=input_fn)\n",
    "print '\\n\\nDone:\\n\\n', done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom tf.contrib.learn Model (model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': None, '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11b217650>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmplj2BkB\n",
      "\n",
      "\n",
      "Fitting (train) estimator:\n",
      "\n",
      "\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmplj2BkB/model.ckpt.\n",
      "INFO:tensorflow:loss = 28.7540333712, step = 1\n",
      "INFO:tensorflow:global_step/sec: 990.019\n",
      "INFO:tensorflow:loss = 0.262683448884, step = 101 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 919.43\n",
      "INFO:tensorflow:loss = 0.0208926150453, step = 201 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 994.918\n",
      "INFO:tensorflow:loss = 0.0029207597063, step = 301 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 939.452\n",
      "INFO:tensorflow:loss = 0.000357327770942, step = 401 (0.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 929.869\n",
      "INFO:tensorflow:loss = 1.86234494058e-05, step = 501 (0.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 1039.23\n",
      "INFO:tensorflow:loss = 1.23846317093e-06, step = 601 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 819.458\n",
      "INFO:tensorflow:loss = 6.3670145578e-08, step = 701 (0.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 1164.27\n",
      "INFO:tensorflow:loss = 8.87833494053e-09, step = 801 (0.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 980.69\n",
      "INFO:tensorflow:loss = 2.39985406841e-09, step = 901 (0.102 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmplj2BkB/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 9.56469489468e-11.\n",
      "\n",
      "\n",
      "Evaluating (predict) estimator:\n",
      "\n",
      "\n",
      "INFO:tensorflow:Starting evaluation at 2017-05-27-18:56:09\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmplj2BkB/model.ckpt-1000\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2017-05-27-18:56:10\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 1.00359e-10\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "\n",
      "\n",
      "Done:\n",
      "\n",
      "{'loss': 1.0035948e-10, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "# tf.contrib.learn does not lock you into its predefined models. Suppose we wanted to create a custom model\n",
    "# that is not built into TensorFlow. We can still retain the high level abstraction of data set, feeding,\n",
    "# training, etc. of tf.contrib.learn. \n",
    "\n",
    "# For illustration, we will show how to implement our own equivalent model\n",
    "# to LinearRegressor using our knowledge of the lower level TensorFlow API.\n",
    "\n",
    "# To define a custom model that works with tf.contrib.learn, we need to use tf.contrib.learn.Estimator.\n",
    "# Because tf.contrib.learn.LinearRegressor is actually a sub-class of tf.contrib.learn.Estimator,\n",
    "# instead of sub-classing Estimator, we simply provide Estimator a function model_fn that tells\n",
    "# tf.contrib.learn how it can evaluate predictions, training steps, and loss (to mimic the exact behavior\n",
    "# of a .LinearRegressor).\n",
    "\n",
    "# The code is as follows:\n",
    "\n",
    "# Declare list of features, we only have one real-valued feature.\n",
    "features = [tf.contrib.layers.real_valued_column(\"x\", dimension=1)]\n",
    "\n",
    "def model(features, labels, mode):\n",
    "    '''cf. Closure `feval(...)` in optim.sgd input for Torch.'''\n",
    "    # Build a linear model and predict values\n",
    "    W = tf.get_variable(\"W\", [1], dtype=tf.float64) # Gets an existing variable w/ these params or creates a new one.\n",
    "    b = tf.get_variable(\"b\", [1], dtype=tf.float64)\n",
    "    y = W * features['x'] + b # predcict\n",
    "    \n",
    "    # Loss sub-graph\n",
    "    loss = tf.reduce_sum(tf.square(y - labels))\n",
    "    \n",
    "    # Training sub-graph\n",
    "    global_step = tf.train.get_global_step() # global_step value\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "    train = tf.group(optimizer.minimize(loss),      # tf.group(*inputs):\n",
    "                   tf.assign_add(ref=global_step,   #\n",
    "                                 value=1))          # Create an op that groups multiple operations.\n",
    "                                                    # When this op finishes, all ops in `input` have finished.\n",
    "                                                    # This op has no output.\n",
    "                                                    #\n",
    "                                                    # tf.assign_add: Update 'ref' by adding 'value' to it.\n",
    "                                                    # (we must manually increment global_step value at each step)\n",
    "    # ModelFnOps connects the subgraphs that we built to the\n",
    "    # appropriate functionality.\n",
    "    return tf.contrib.learn.ModelFnOps(\n",
    "      mode=mode,\n",
    "      loss=loss,\n",
    "      train_op=train)\n",
    "\n",
    "estimator = tf.contrib.learn.Estimator(model_fn=model)\n",
    "\n",
    "# define our data set\n",
    "x = np.array([1., 2., 3., 4.])\n",
    "y = np.array([0., -1., -2., -3.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x}, y, 4, num_epochs=1000)\n",
    "\n",
    "# train\n",
    "print '\\n\\nFitting (train) estimator:\\n\\n'\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "\n",
    "# evaluate our model\n",
    "print '\\n\\nEvaluating (predict) estimator:\\n\\n'\n",
    "done = estimator.evaluate(input_fn=input_fn, steps=10)\n",
    "print '\\n\\nDone:\\n\\n', done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
